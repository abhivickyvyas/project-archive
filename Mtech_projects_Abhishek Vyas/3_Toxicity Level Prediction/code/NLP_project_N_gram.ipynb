{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP project  N-gram.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFtMSlvYf4Tn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec75a084-8c32-4863-a77d-36c909a186b2"
      },
      "source": [
        "# import math\n",
        "# from nltk.tokenize import RegexpTokenizer\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# wordnet_lemmatizer = WordNetLemmatizer()\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "import math\n",
        "import json\n",
        "# import numpy  as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "Vocab={}\n",
        "n_gramlist_fequency={}\n",
        "starting_word_of_sentence={}\n",
        "def smoothening(n_gramlist_fequency):\n",
        "    sz_Vocab=len(Vocab)\n",
        "    all_smoothen={}\n",
        "    laPlace_smoothing_unigram={}\n",
        "    laPlace_smoothing_bigram={}\n",
        "    laPlace_smoothing_trigram={}\n",
        "    val=sum(n_gramlist_fequency['unigram'].values())\n",
        "    for x,y in n_gramlist_fequency['unigram'].items():\n",
        "#         value = ((n_gramlist_fequency['unigram'][x]+1)/(val+sz_Vocab))\n",
        "        value = ((y+1)/(val+sz_Vocab))\n",
        "        laPlace_smoothing_unigram[x]=math.log10(value)\n",
        "    all_smoothen['unigram']=laPlace_smoothing_unigram\n",
        "    \n",
        "    for x1,y1 in n_gramlist_fequency['bigram'].items():\n",
        "        ew1=x1.split()\n",
        "        key1 = ew1[1]+'|'+ew1[0]\n",
        "        value1=((y1+1)/(n_gramlist_fequency['unigram'][ew1[0]]+sz_Vocab))\n",
        "        laPlace_smoothing_bigram[key1]=math.log10(value1)\n",
        "    all_smoothen['bigram']=laPlace_smoothing_bigram\n",
        "    for x2,y2 in n_gramlist_fequency['trigram'].items():\n",
        "        ew2=x2.split()\n",
        "        key2 = ew2[2]+'|'+ew2[0]+' '+ew2[1]\n",
        "        value2=((y2+1)/(n_gramlist_fequency['bigram'][ew2[0]+' '+ew2[1]]+sz_Vocab))\n",
        "        laPlace_smoothing_trigram[key2]=math.log10(value2)\n",
        "    all_smoothen['trigram']=laPlace_smoothing_trigram    \n",
        "    return all_smoothen\n",
        "def create_n_igramlist(list,n):\n",
        "    unigram=list\n",
        "    bigram=[]\n",
        "    trigram=[]\n",
        "    for k in range(len(unigram)-1):\n",
        "        bigram.append(unigram[k]+' '+unigram[k+1])\n",
        "    for j in range(len(unigram)-2):\n",
        "        trigram.append(unigram[j]+' '+unigram[j+1]+' '+unigram[j+2])\n",
        "    ngrams={}\n",
        "    ngrams['unigram']=unigram\n",
        "    ngrams['bigram']=bigram\n",
        "    ngrams['trigram']=trigram\n",
        "    return ngrams\n",
        "def frequency_of_words_in_list(ls):\n",
        "    freq={}\n",
        "    for i in ls:\n",
        "        if i in freq:\n",
        "            freq[i]+=1\n",
        "        else:\n",
        "            freq[i]=1\n",
        "    return freq\n",
        "def tokeninfolderfile(comment_in_class):\n",
        "    l=[]\n",
        "    n_gramlist={}\n",
        "    global Vocab\n",
        "    n_gramlist_fequency={}\n",
        "    n_gramlist_fequency['unigram']={}\n",
        "    n_gramlist_fequency['bigram']={}\n",
        "    n_gramlist_fequency['trigram']={}\n",
        "    \n",
        "    n_gramlist['unigram']=[]\n",
        "    n_gramlist['bigram']=[]\n",
        "    n_gramlist['trigram']=[]\n",
        "    \n",
        "    for comment in comment_in_class:        \n",
        "        tk = RegexpTokenizer(r'[\\w]+[-\\']?[\\w]*')   \n",
        "        newtokens=tk.tokenize(comment)\n",
        "        new_words=[] \n",
        "        for w in newtokens:\n",
        "                w=wordnet_lemmatizer.lemmatize(w.lower())\n",
        "                new_words.append(w)   \n",
        "                if w in Vocab:\n",
        "                    Vocab[w]+=1\n",
        "                else:\n",
        "                    Vocab[w]=1                        \n",
        "        n_g_list=create_n_igramlist(new_words,0)\n",
        "        n_gramlist['unigram'].extend(n_g_list['unigram'])\n",
        "        n_gramlist['bigram'].extend(n_g_list['bigram'])\n",
        "        n_gramlist['trigram'].extend(n_g_list['trigram'])\n",
        "            \n",
        "    n_gramlist_fequency['unigram']=frequency_of_words_in_list(n_gramlist['unigram'])\n",
        "#     print(len(n_gramlist_fequency['unigram']))\n",
        "#     print(len(Vocab))\n",
        "    n_gramlist_fequency['bigram']=frequency_of_words_in_list(n_gramlist['bigram'])\n",
        "    n_gramlist_fequency['trigram']=frequency_of_words_in_list(n_gramlist['trigram'])   \n",
        "#         l.extend(new_words)\n",
        "#     return l\n",
        "    return n_gramlist_fequency"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyjGgzVUf4Tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('jigsaw-toxic-comment-classification-challenge/train.csv')\n",
        "cmnt_text = list(data['comment_text'])\n",
        "toxic = list(data['toxic'])\n",
        "st = list(data['severe_toxic'])\n",
        "ob = list(data['obscene'])\n",
        "th = list(data['threat'])\n",
        "ins = list(data['insult'])\n",
        "ih = list(data['identity_hate'])\n",
        "toxic_comments = []\n",
        "severe_toxic_comments = []\n",
        "obscene_comments = []\n",
        "threat_comments = []\n",
        "insult_comments = []\n",
        "identity_hate_comments = []\n",
        "\n",
        "for i in range(0,len(toxic)):\n",
        "    if(toxic[i] == 1):\n",
        "        toxic_comments+=[cmnt_text[i]]\n",
        "        \n",
        "for i in range(0,len(st)):\n",
        "    if(st[i] == 1):\n",
        "        severe_toxic_comments+=[cmnt_text[i]]\n",
        "        \n",
        "for i in range(0,len(ob)):\n",
        "    if(ob[i] == 1):\n",
        "        obscene_comments+=[cmnt_text[i]]\n",
        "        \n",
        "for i in range(0,len(th)):\n",
        "    if(th[i] == 1):\n",
        "        threat_comments+=[cmnt_text[i]]\n",
        "\n",
        "for i in range(0,len(ins)):\n",
        "    if(ins[i] == 1):\n",
        "        insult_comments+=[cmnt_text[i]]\n",
        "        \n",
        "for i in range(0,len(ih)):\n",
        "    if(ih[i] == 1):\n",
        "        identity_hate_comments+=[cmnt_text[i]]\n",
        "dict_comments = {\"toxic\":toxic_comments,\"severe_toxic\":severe_toxic_comments, \"obscene\":obscene_comments, \"threat\":threat_comments, \"insult\":insult_comments, \"identity_hate\":identity_hate_comments }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XUt5t9yf4T8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_per_word_per_class={}\n",
        "for i,j in dict_comments.items():\n",
        "    f_per_word_per_class[i]={}\n",
        "    f_per_word_per_class[i]=tokeninfolderfile(j)\n",
        "    \n",
        "# print(f_per_word_per_class['toxic'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWsfPQ2yf4UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smothen={}\n",
        "for i,j in f_per_word_per_class.items():\n",
        "    smothen[i]={}\n",
        "    smothen[i]=smoothening(j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "NAYrgCVUf4UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_probability_per_class(f_per_word_per_class,smothen,yoursentence):\n",
        "        global Vocab\n",
        "        tk = RegexpTokenizer(r'[\\w]+[-\\']?[\\w]*')\n",
        "        # yoursentence=input(\"Enter a valid sentence\\n\")\n",
        "        # yoursentence=\"abcdbcbcbcbbcbcbbc cbcbbcbbcbbcbbc bcbcbcbbcbbcbcbjshksdkaskd cjakcjskjhckjsc When I got my knee rebuilt I got back on the street bike ASAP. I put the crutches on the rack and the passenger seat and they hung out back a LONG way. Just make sure they're tied down tight in front and no problemo.\"\n",
        "#         yoursentence=\"fuck you ash hole mother fucker you dick is too dusty\"\n",
        "        newtokens=tk.tokenize(yoursentence)\n",
        "        new_words=[]\n",
        "        for w in newtokens:\n",
        "                w=wordnet_lemmatizer.lemmatize(w.lower())   \n",
        "                new_words.append(w)\n",
        "        n_g_list=create_n_igramlist(new_words,0)\n",
        "        prob_of_sent_per_class_per_gram={}\n",
        "        for toxic_class,toxic_class_data in smothen.items():\n",
        "            prob_of_sent_per_class_per_gram[toxic_class]={}\n",
        "            for n_gram,n_gram_values in toxic_class_data.items():\n",
        "                if(n_gram==\"unigram\"):\n",
        "                    Unigram_prob_of_sen=0\n",
        "                    for w in n_g_list['unigram']:\n",
        "                        if(w in smothen[toxic_class]['unigram']):\n",
        "                            Unigram_prob_of_sen=Unigram_prob_of_sen+smothen[toxic_class]['unigram'][w]\n",
        "                        else:\n",
        "                            Unigram_prob_of_sen=Unigram_prob_of_sen+(math.log10(1/(sum(f_per_word_per_class[toxic_class]['unigram'].values())+(len(smothen[toxic_class]['unigram'])))))  \n",
        "                    prob_of_sent_per_class_per_gram[toxic_class][n_gram]=Unigram_prob_of_sen\n",
        "                elif(n_gram==\"bigram\"):\n",
        "                    Bigram_prob_of_sen=0\n",
        "                    for w in n_g_list['bigram']:\n",
        "                        split_word=w.split()\n",
        "                        key = split_word[1]+'|'+split_word[0]\n",
        "                        sz_Vocab=len(Vocab)\n",
        "                    #     print(sz_Vocab)\n",
        "                        if(key in smothen[toxic_class]['bigram']):\n",
        "                            Bigram_prob_of_sen=Bigram_prob_of_sen+smothen[toxic_class]['bigram'][key]\n",
        "                        else:\n",
        "                            if(split_word[0] in f_per_word_per_class[toxic_class]['unigram'].keys()):\n",
        "                                unigrampresent=f_per_word_per_class[toxic_class]['unigram'][split_word[0]]\n",
        "                            else:\n",
        "                                unigrampresent=0\n",
        "                            Bigram_prob_of_sen=Bigram_prob_of_sen+math.log10(1/(unigrampresent+sz_Vocab))\n",
        "                    prob_of_sent_per_class_per_gram[toxic_class][n_gram]=Bigram_prob_of_sen\n",
        "\n",
        "                elif(n_gram==\"trigram\"):\n",
        "                    trigram_prob_of_sen=0\n",
        "                    for w in n_g_list['trigram']:\n",
        "                        split_word=w.split()\n",
        "                        key = split_word[2]+'|'+split_word[0]+' '+split_word[1]\n",
        "                        if(key in smothen[toxic_class]['trigram']):\n",
        "                    #         print(des_laPlace_smoothing_trigram[key])\n",
        "                            trigram_prob_of_sen=trigram_prob_of_sen+smothen[toxic_class]['trigram'][key]\n",
        "                        else:\n",
        "                            prev_bigramword=split_word[0]+' '+split_word[1]\n",
        "                            if(prev_bigramword in f_per_word_per_class[toxic_class]['bigram'].keys()):\n",
        "                                pre_bigrampresent=f_per_word_per_class[toxic_class]['bigram'][prev_bigramword]\n",
        "                            else:\n",
        "                                pre_bigrampresent=0\n",
        "                            trigram_prob_of_sen=trigram_prob_of_sen+math.log10(1/(pre_bigrampresent+sz_Vocab))\n",
        "                    prob_of_sent_per_class_per_gram[toxic_class][n_gram]=trigram_prob_of_sen\n",
        "        return prob_of_sent_per_class_per_gram\n",
        "            \n",
        "prob_per_class_sent={}\n",
        "yoursentence=\"fuck you ash hole mother fucker you dick is too dusty\"\n",
        "prob_per_class_sent=sentence_probability_per_class(f_per_word_per_class,smothen,yoursentence)\n",
        "# print(prob_per_class_sent)\n",
        "# for toxic_class,toxic_class_data in prob_per_class_sent.items():\n",
        "#     print(toxic_class)\n",
        "#     for n_gram,n_gram_values in toxic_class_data.items():\n",
        "#         print(n_gram,n_gram_values)\n",
        "#     print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cepqnc2qf4UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #read text data\n",
        "# data = pd.read_csv('jigsaw-toxic-comment-classification-challenge/test.csv')\n",
        "# # cmnt_text = list(data['comment_text'])\n",
        "# per_comment_prob={}\n",
        "# for y,x in enumerate(list(data['comment_text'])):\n",
        "#     per_comment_prob[data['id'][y]]=sentence_probability_per_class(f_per_word_per_class,smothen,x)\n",
        "        \n",
        "# #     per_comment_prob[]sentence_probability_per_class(f_per_word_per_class,smothen,x)\n",
        "# # per_comment_prob   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdml0sJCf4Uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# def storeData(filename,datais):\n",
        "#     dbfile = open(filename, 'ab')\n",
        "#     pickle.dump(datais, dbfile)\n",
        "#     dbfile.close()   \n",
        "# def loadData(filename):\n",
        "#     dbfile = open(filename, 'rb')\n",
        "#     datais=pickle.load(dbfile)\n",
        "#     dbfile.close()\n",
        "#     return datais;\n",
        "# storeData('test_data_of_all',per_comment_prob)    \n",
        "# print(\"done\")\n",
        "# # print(loadData('test_data_of_1000'))\n",
        "# # print(len(per_comment_prob))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5yMOTovf4Up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "def storeData(filename,datais):\n",
        "    dbfile = open(filename, 'ab')\n",
        "    pickle.dump(datais, dbfile)\n",
        "    dbfile.close()\n",
        "\n",
        "    \n",
        "def loadData(filename):\n",
        "    dbfile = open(filename, 'rb')\n",
        "    datais=pickle.load(dbfile)\n",
        "    dbfile.close()\n",
        "    return datais\n",
        "abc={}\n",
        "abc=loadData('test_data_of_all')\n",
        "k={}\n",
        "label_data = pd.read_csv('jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n",
        "list_of_file_to_find_ac=[]\n",
        "for x,y in enumerate(label_data['id']):\n",
        "#         if(x<1000):\n",
        "            if((label_data[\"toxic\"][x] == 1 or label_data[\"severe_toxic\"][x] == 1 or  label_data[\"obscene\"][x] == 1 or label_data[\"threat\"][x] == 1 or label_data[\"insult\"][x] == 1 or label_data[\"identity_hate\"][x] == 1 )):\n",
        "                list_of_file_to_find_ac.append(y)\n",
        "\n",
        "uni_max_val=-9223372036854775808\n",
        "bi_max_val=-9223372036854775808\n",
        "tri_max_val=-9223372036854775808\n",
        "max_uni=''\n",
        "max_bi=''\n",
        "max_tri=''\n",
        "sentence_belong_to_class={}\n",
        "for x,y in abc.items():\n",
        "#     print(y)\n",
        "    if(x in list_of_file_to_find_ac):\n",
        "        pqr={}\n",
        "        for toxic_class,toxic_class_data in y.items():\n",
        "            if(toxic_class_data['unigram']>uni_max_val):\n",
        "                uni_max_val=toxic_class_data['unigram']\n",
        "                max_uni=toxic_class\n",
        "            if(toxic_class_data['bigram']>bi_max_val):\n",
        "                bi_max_val=toxic_class_data['bigram']\n",
        "                max_bi=toxic_class\n",
        "            if(toxic_class_data['trigram']>tri_max_val):\n",
        "                tri_max_val=toxic_class_data['trigram']\n",
        "                max_tri=toxic_class\n",
        "        pqr['unigram']=max_uni\n",
        "        pqr['bigram']=max_bi\n",
        "        pqr['trigram']=max_tri\n",
        "        sentence_belong_to_class[x]={}\n",
        "        sentence_belong_to_class[x]=pqr\n",
        "# print(len(sentence_belong_to_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjyuE4mof4U1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_class=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
        "unigram_class_index_all_files=[]\n",
        "bigram_class_index_all_files=[]\n",
        "trigram_class_index_all_files=[]\n",
        "for x,y in sentence_belong_to_class.items():\n",
        "    unigram_class_index_all_files.append(list_of_class.index(y['unigram']))\n",
        "    bigram_class_index_all_files.append(list_of_class.index(y['bigram']))\n",
        "    trigram_class_index_all_files.append(list_of_class.index(y['trigram']))\n",
        "# print(unigram_class_index_all_files)\n",
        "# print(bigram_class_index_all_files)\n",
        "# print(trigram_class_index_all_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iG4RV1Yf4VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_class=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
        "import pandas as pd\n",
        "label_data = pd.read_csv('jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n",
        "orignal_class_index_all_files=[]\n",
        "for x,y in enumerate(label_data['id']):\n",
        "#     if(x<1000):\n",
        "        if(label_data[\"toxic\"][x] == 1):\n",
        "            orignal_class_index_all_files.append(list_of_class.index(\"toxic\"))\n",
        "        elif(label_data[\"severe_toxic\"][x] == 1):\n",
        "            orignal_class_index_all_files.append(list_of_class.index(\"severe_toxic\"))\n",
        "        elif(label_data[\"obscene\"][x] == 1):\n",
        "            orignal_class_index_all_files.append(list_of_class.index(\"obscene\"))\n",
        "        elif(label_data[\"threat\"][x] == 1):\n",
        "            orignal_class_index_all_files.append(list_of_class.index(\"threat\"))\n",
        "        elif(label_data[\"insult\"][x] == 1):\n",
        "            orignal_class_index_all_files.append(list_of_class.index(\"insult\"))\n",
        "        elif(label_data[\"identity_hate\"][x] == 1):\n",
        "            orignal_class_index_all_files.append(list_of_class.index(\"identity_hate\"))\n",
        "        \n",
        "#     else:\n",
        "#         break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUx07JeVf4VM",
        "colab_type": "code",
        "colab": {},
        "outputId": "265db3cc-8f08-4b8f-e5a5-85bed6f629f7"
      },
      "source": [
        "#unigram_acuracy\n",
        "# print(len(orignal_class_index_all_files))\n",
        "# print(len(unigram_class_index_all_files))\n",
        "# print(len(bigram_class_index_all_files))\n",
        "# print(len(trigram_class_index_all_files))\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"accuracy_score of unigram \",accuracy_score(orignal_class_index_all_files,unigram_class_index_all_files))\n",
        "print(\"accuracy_score of bigram \",accuracy_score(orignal_class_index_all_files,bigram_class_index_all_files))\n",
        "print(\"accuracy_score of trigram \",accuracy_score(orignal_class_index_all_files,trigram_class_index_all_files))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score of unigram  0.15361204549094987\n",
            "accuracy_score of bigram  0.9532276149287202\n",
            "accuracy_score of trigram  0.9753323722569277\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}