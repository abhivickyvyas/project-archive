{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEC2oJ0ldTSX",
        "colab_type": "code",
        "outputId": "7f53cc50-63a5-4121-a6b6-0e9341d96d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import math\n",
        "import json\n",
        "# import numpy  as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# wordnet_lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMDaZdLQu5Y-",
        "colab_type": "code",
        "outputId": "59c5c283-609f-4e2e-c492-d17f2606e90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eThtnGrNUmcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/NLP_project/train.csv')\n",
        "data=data.fillna(\"unknown\")\n",
        "cmnt_text = list(data['comment_text'])\n",
        "toxic = list(data['toxic'])\n",
        "st = list(data['severe_toxic'])\n",
        "ob = list(data['obscene'])\n",
        "th = list(data['threat'])\n",
        "ins = list(data['insult'])\n",
        "ih = list(data['identity_hate'])\n",
        "toxic_comments = []\n",
        "severe_toxic_comments = []\n",
        "obscene_comments = []\n",
        "threat_comments = []\n",
        "insult_comments = []\n",
        "identity_hate_comments = []\n",
        "\n",
        "for i in range(0,len(toxic)):\n",
        "    if(toxic[i] == 1):\n",
        "        toxic_comments+=[cmnt_text[i]]\n",
        "        \n",
        "for i in range(0,len(st)):\n",
        "    if(st[i] == 1):\n",
        "        severe_toxic_comments+=[cmnt_text[i]]\n",
        "        \n",
        "for i in range(0,len(ob)):\n",
        "    if(ob[i] == 1):\n",
        "        obscene_comments+=[cmnt_text[i]]\n",
        "        \n",
        "for i in range(0,len(th)):\n",
        "    if(th[i] == 1):\n",
        "        threat_comments+=[cmnt_text[i]]\n",
        "\n",
        "for i in range(0,len(ins)):\n",
        "    if(ins[i] == 1):\n",
        "        insult_comments+=[cmnt_text[i]]\n",
        "        \n",
        "for i in range(0,len(ih)):\n",
        "    if(ih[i] == 1):\n",
        "        identity_hate_comments+=[cmnt_text[i]]\n",
        "dict_comments = {\"toxic\":toxic_comments,\"severe_toxic\":severe_toxic_comments, \"obscene\":obscene_comments, \"threat\":threat_comments, \"insult\":insult_comments, \"identity_hate\":identity_hate_comments }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHEFp5dSUwGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_sent(listis):\n",
        "  listToStr = ' '.join([str(elem) for elem in listis]) \n",
        "  return listToStr\n",
        "\n",
        "kis=list_to_sent(dict_comments['toxic'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh8My0UzZZAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listofsent=[]\n",
        "for cls,temp in dict_comments.items():\n",
        "      listofsent.append(list_to_sent(temp))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVErv2h1adQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# listofsent[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAAlIA7DXQxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_sent_for_cosine={}\n",
        "freq_in_doc={} \n",
        "porter = PorterStemmer() \n",
        "word_freq_per_sent={} \n",
        "count=0\n",
        "tk = RegexpTokenizer(r'[\\w]+[-\\']?[\\w]*')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# for y,x in enumerate(list(data['comment_text'])):\n",
        "# listofsent=[]\n",
        "for y,p in enumerate(listofsent):\n",
        "#     print(y,p,\"\\n\")\n",
        "    word_freq_per_sent[y]={}\n",
        "    list_of_sent_for_cosine[y]=[]\n",
        "    \n",
        "#     tk = RegexpTokenizer(r'[\\w]+[-\\']?[\\w]*')    \n",
        "    newtokens=tk.tokenize(p)\n",
        "    # print(newtokens)\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "    for w in newtokens:\n",
        "#         w=w.lower()\n",
        "        w=porter.stem(w.lower())\n",
        "#         w=wordnet_lemmatizer.lemmatize(w.lower())\n",
        "        if w not in stop_words:\n",
        "            list_of_sent_for_cosine[y].append(w)\n",
        "            if w in word_freq_per_sent[y]:\n",
        "                word_freq_per_sent[y][w]+=1\n",
        "            else:\n",
        "                word_freq_per_sent[y][w]=1    \n",
        "            if w in freq_in_doc:\n",
        "                freq_in_doc[w].add(y)\n",
        "            else:\n",
        "                freq_in_doc[w]={y}\n",
        "            # if w in Vocab:\n",
        "            #     Vocab[w]+=1\n",
        "            # else:\n",
        "            #     Vocab[w]=1\n",
        "for w_is in freq_in_doc:\n",
        "    freq_in_doc[w_is]=len(freq_in_doc[w_is])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1P-p9pjbY-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_train_sent=list(list_of_sent_for_cosine.values())\n",
        "list_of_train_sent_tags=list(list_of_sent_for_cosine.keys())\n",
        "# list_of_train_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTq5PMV_dmvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine(v1, v2):\n",
        "    v1 = np.array(v1)\n",
        "    v2 = np.array(v2)\n",
        "    return np.dot(v1, v2) / (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFVp_XiYbc_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_tf(snt_word_count):\n",
        "    tf={}\n",
        "    len_of_doc=sum(snt_word_count.values())\n",
        "#     print(sum(snt_word_count.values()))  \n",
        "    for x,y in snt_word_count.items():\n",
        "        tf[x]=1+math.log(1+y)#y/len_of_doc\n",
        "    return tf  \n",
        "    \n",
        "def compute_idf(fr_in_doc,lenis):\n",
        "    idf={}\n",
        "#     print(fr_in_doc)\n",
        "#     print(lenis)  \n",
        "    for x,y in fr_in_doc.items():\n",
        "#         print(x,y)\n",
        "        idf[x]=math.log(lenis/(1+y))\n",
        "    return idf      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J79LnByZbnhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "term_freq_dict={}\n",
        "for x,y in word_freq_per_sent.items():\n",
        "#     print(y)\n",
        "    term_freq_dict[x]=compute_tf(y)\n",
        "# term_freq_dict    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIDxTG-Mb8X0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "invrs_doc_freq_dict=compute_idf(freq_in_doc,len(listofsent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdmPqSQNbsjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_tf_idf(tf,idf,lenis):\n",
        "#     print(tf,\"\\n\\n\\n====================\")\n",
        "#     print(idf)\n",
        "    vector_list=[]\n",
        "    tf_idf_dict={}\n",
        "    for p,q in tf.items():\n",
        "        if(p in idf.keys()):\n",
        "#             print(p,q,idf[p])\n",
        "#             print(p,tf[p],y)\n",
        "            vector_list.append(q*idf[p])\n",
        "            tf_idf_dict[p]=q*idf[p]\n",
        "        else:\n",
        "            vector_list.append(q*math.log(lenis))\n",
        "            tf_idf_dict[p]=q*math.log(lenis)\n",
        "    return vector_list,tf_idf_dict   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHkqY7PebyKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "term_freq_invrs_doc_freq_dic={}\n",
        "term_freq_invrs_doc_freq_dic_list={}\n",
        "for indx,k in term_freq_dict.items():\n",
        "    term_freq_invrs_doc_freq_dic_list[indx],term_freq_invrs_doc_freq_dic[indx]=compute_tf_idf(k,invrs_doc_freq_dict,len(listofsent))\n",
        "\n",
        "#     print(k,\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmzOyZhHcoJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepocess(data):\n",
        "#     tk = RegexpTokenizer(r'[\\w]+[-\\']?[\\w]*')\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "    newtokens1=tk.tokenize(data)\n",
        "#     print(newtokens1)\n",
        "    #     stop_words = set(stopwords.words('english'))\n",
        "#     sent_is=[]\n",
        "    word_freq_per_sent_is={}\n",
        "    for w1 in newtokens1:\n",
        "#         w1=wordnet_lemmatizer.lemmatize(w1.lower())\n",
        "        w1=porter.stem(w1.lower())\n",
        "        if w1 not in stop_words:\n",
        "            if w1 in word_freq_per_sent_is:\n",
        "                word_freq_per_sent_is[w1]+=1\n",
        "            else:\n",
        "                word_freq_per_sent_is[w1]=1\n",
        "#             sent_is.append(w1)\n",
        "    return word_freq_per_sent_is "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PilehvQVckuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine(v1, v2):\n",
        "    v1 = np.array(v1)\n",
        "    v2 = np.array(v2)\n",
        "\n",
        "    return np.dot(v1, v2) / (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuY_zQuPg_hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqp-BYylg5QH",
        "colab_type": "code",
        "outputId": "6a4ff242-5174-440d-c4d7-6ca313fa4056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def per_query_sol(sentis,to_tf_idf_dict,idf_train,len_train_sent):   \n",
        "    # print(sentis)\n",
        "    option_preprocess=prepocess(sentis)\n",
        "    dictis={}\n",
        "    tf_of_option_preprocess=compute_tf(option_preprocess)\n",
        "    # print(tf_of_option_preprocess)\n",
        "    tf_idf_of_=compute_tf_idf(tf_of_option_preprocess,idf_train,len_train_sent)\n",
        "    # print(tf_idf_of_)\n",
        "    max_score_doc=0\n",
        "    max_classis=\"\"\n",
        "    for x,y in to_tf_idf_dict.items():\n",
        "      # print(x,y) \n",
        "      dictis[x]=0\n",
        "      intersection_list=list(np.intersect1d(list(tf_of_option_preprocess.keys()),list(y.keys())))\n",
        "      if(len(intersection_list)>0):\n",
        "            # print(intersection_list)\n",
        "            # print(len(intersection_list))\n",
        "      # print(\"\\n\\n\")\n",
        "            new_tf_of_option_preprocess = { key: tf_of_option_preprocess[key] for key in intersection_list }\n",
        "            new_y={ key1: y[key1] for key1 in intersection_list }\n",
        "# #                 print(new_tf_of_option_preprocess,new_y)\n",
        "            vec1_of_op,tf_idf_of_option_preprocess=compute_tf_idf(new_tf_of_option_preprocess,idf_train,len_train_sent)\n",
        "            vec2_of_train_doc=list(new_y.values())\n",
        "            # print(vec1_of_op,vec2_of_train_doc)\n",
        "            cosine_sim=cosine(vec1_of_op,vec2_of_train_doc)\n",
        "            dictis[x]=cosine_sim\n",
        "            # print(cosine_sim)\n",
        "      # print(\"\\n\\n\\n\")\n",
        "# #                 print(tf_idf_of_option_preprocess,new_y)\n",
        "            if(cosine_sim>max_score_doc):\n",
        "                    max_score_doc=cosine_sim\n",
        "                    max_classis=x\n",
        "    return max_classis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_data['comment_text'][0]=\"== Arabs are committing genocide in Iraq, but ...\"\n",
        "per_query_sol(test_data['comment_text'][0],term_freq_invrs_doc_freq_dic,invrs_doc_freq_dict,len(listofsent))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC9HmL16h8TU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classification_data['imdb_score']=classification_data.apply(lambda row:'D' if ( int(row.imdb_score)>=0 and int(row.imdb_score)<5) else ('C' if (int(row.imdb_score)>=5 and int(row.imdb_score)<6) else ('B' if (int(row.imdb_score)>=6 and int(row.imdb_score)<7) else ('A' if (int(row.imdb_score)>=7 and int(row.imdb_score)<8) else ('A++' if (int(row.imdb_score)>=8 and int(row.imdb_score)<=10) else 0) )))  , axis = 1)\n",
        "  \n",
        "# new_test_data['real_label']=new_test_data.apply(lambda row:0 if (row.toxic==1) else (1 if (row.severe_toxic==1) else (2 if (row.obscene==1) else (3 if (row.threat==1) else (4 if (row.insult==1) else (5 if (row.identity_hate==1) else 0)) )))  , axis = 1)\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXKfQbYlmXvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_data = pd.read_csv('/content/drive/My Drive/NLP_project/test_labels.csv')\n",
        "list_of_file_to_find_ac=[]\n",
        "for x,y in enumerate(label_data['id']):\n",
        "#         if(x<1000):\n",
        "            if((label_data[\"toxic\"][x] == 1 or label_data[\"severe_toxic\"][x] == 1 or  label_data[\"obscene\"][x] == 1 or label_data[\"threat\"][x] == 1 or label_data[\"insult\"][x] == 1 or label_data[\"identity_hate\"][x] == 1 )):\n",
        "                list_of_file_to_find_ac.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwKxMojIu0J3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "999fbf7c-95ad-426c-ea8d-621189e7b166"
      },
      "source": [
        "len(list_of_file_to_find_ac)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-jNPNnH_dN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for k in list(test_data['comment_text']):\n",
        "# dictIS=test_data\n",
        "# len(test_data)\n",
        "label_data = pd.read_csv('/content/drive/My Drive/NLP_project/test_labels.csv')\n",
        "new_test_data=label_data\n",
        "  \n",
        "new_test_data['real_label']=new_test_data.apply(lambda row:0 if (row.toxic==1) else (1 if (row.severe_toxic==1) else (2 if (row.obscene==1) else (3 if (row.threat==1) else (4 if (row.insult==1) else (5 if (row.identity_hate==1) else 5000)) )))  , axis = 1)\n",
        "\n",
        "new_test_data = new_test_data[new_test_data['real_label'] !=5000]\n",
        "\n",
        "# list_of_file_to_find_ac=[]\n",
        "# for x,y in enumerate(label_data['id']):\n",
        "# #         if(x<1000):\n",
        "#       # print(x,y)\n",
        "#       # break\n",
        "\n",
        "#             if((label_data[\"toxic\"][x] == 1 or label_data[\"severe_toxic\"][x] == 1 or  label_data[\"obscene\"][x] == 1 or label_data[\"threat\"][x] == 1 or label_data[\"insult\"][x] == 1 or label_data[\"identity_hate\"][x] == 1 )):\n",
        "#                 list_of_file_to_find_ac.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8b5tydEo85L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_datais= pd.read_csv('/content/drive/My Drive/NLP_project/test.csv')\n",
        "\n",
        "new_test_data_withsent=pd.merge(test_datais, new_test_data, on='id')\n",
        "new_test_data_withsent\n",
        "\n",
        "for col in [\"toxic\",\"severe_toxic\",\"threat\",\"insult\",\"identity_hate\",\"obscene\"]:\n",
        "  if (col in new_test_data_withsent.columns):\n",
        "      new_test_data_withsent=new_test_data_withsent.drop(columns=[col])\n",
        "# new_test_data_withsent\n",
        "# new_test_data_withsent\n",
        "\n",
        "# per_query_sol(test_data['comment_text'][0],term_freq_invrs_doc_freq_dic,invrs_doc_freq_dict,len(listofsent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7ndIEEHrrm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_test_data_withsent['found_label']=new_test_data_withsent.apply(lambda row:per_query_sol(row.comment_text,term_freq_invrs_doc_freq_dic,invrs_doc_freq_dict,len(listofsent)))\n",
        "# new_test_data_withsent\n",
        "# len(new_test_data_withsent)\n",
        "new_test_data_withsent.loc[:,'found_label'] = new_test_data_withsent.apply(lambda x: per_query_sol(x['comment_text'],term_freq_invrs_doc_freq_dic,invrs_doc_freq_dict,len(listofsent)), axis =1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpk4NGpmyDGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# print(\"accuracy_score of tf-idf \",accuracy_score(list(new_test_data_withsent['real_label']),list(new_test_data_withsent['found_label'])))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}