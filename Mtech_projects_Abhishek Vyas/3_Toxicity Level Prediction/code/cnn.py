# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UwBLbvh_A86MLtGF3p-oals0_PenDZEI
"""



from google.colab import drive

drive.mount('/content/drive')

import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences as pad
from keras.layers import Dense, Input, Embedding, Dropout, Activation,Conv1D,MaxPooling1D
from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional
from keras.models import Model
from keras import layers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
def find_max(arr):
  max_el = arr[0]
  for i in arr:
    if(i>max_el):
      max_el = i
  return arr.index(max_el) 
data = pd.read_csv("/content/drive/My Drive/NLP/train.csv")
t_data = pd.read_csv("/content/drive/My Drive/NLP/test.csv")
t_label = pd.read_csv("/content/drive/My Drive/NLP/test_labels.csv")
print(data.head(10))
X_train, X_test, y_train, y_test = train_test_split(data, data[["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]], test_size = 0.10, random_state = 42)
list_sent_train = X_train["comment_text"]
list_sent_test = X_test["comment_text"]
sent_text = t_data['comment_text']
l_toxic = t_label['toxic']
l_sever_t = t_label['severe_toxic']
l_obscene = t_label['obscene']
l_idh = t_label['identity_hate']
l_in = t_label['insult']
l_th = t_label['threat']
max_features = 20000
tokenizer = Tokenizer(num_words=max_features,char_level=True)
tokenizer.fit_on_texts(list(list_sent_train))
list_token_train = tokenizer.texts_to_sequences(list_sent_train)
list_sent_test = tokenizer.texts_to_sequences(list_sent_test)
maxlength = 500
X_t = pad(list_token_train, maxlen=maxlength)
X_te = pad(list_sent_test, maxlen=maxlength)
inp = Input(shape=(maxlength, ))

embedding_size = 240
x = Embedding(len(tokenizer.word_index)+1, embedding_size)(inp)

x = Conv1D(filters=100,kernel_size=4,padding='same', activation='relu')(x)

x=MaxPooling1D(pool_size=4)(x)

#x = Bidirectional(GRU(60, return_sequences=True,name='lstm_layer',dropout=0.2,recurrent_dropout=0.2))(x)

x = GlobalMaxPool1D()(x)

x = Dense(50, activation="relu")(x)
x = Dropout(0.2)(x)
x = Dense(6, activation="sigmoid")(x)

model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                 metrics=['accuracy'])

print(model.summary())
batch_size = 32
epochs = 5
hist = model.fit(X_t,y_train, batch_size=batch_size, epochs=epochs,validation_data=(X_te,y_test))
original = []
predicted = []
for i,j in enumerate(sent_text) :
    flag = 0
    if(l_toxic[i] == 1):
      original.append(0)
    elif(l_sever_t[i] == 1):
      original.append(1)
    elif(l_obscene[i]==1):
      original.append(2)
    elif(l_idh[i] == 1):
      original.append(3)
    elif(l_in[i]==1):
      original.append(4)
    elif(l_th[i] == 1):
      original.append(5)
    else:
      flag =1
      continue
    if(flag == 0):
      sent = j
      sent = tokenizer.texts_to_sequences(sent)
      sent = pad_sequences(sent, maxlen=500, padding='post', truncating='post')
      prediction = model.predict(sent)
      values = [prediction[0][0],prediction[0][1],prediction[0][2],prediction[0][3],prediction[0][4],prediction[0][5]]
      ind = find_max(values)
      predicted.append(ind)
print(accuracy_score(original,predicted))

original = []
for i,j in enumerate(list_sentences_train) :
    sent = j
    sent = tokenizer.texts_to_sequences(sent)
    sent = pad_sequences(sent, maxlen=500, padding='post', truncating='post')
    prediction = model.predict(sent)
    
    # Print output
    print("Toxicity levels for scentence : ",i)
    print('Toxic:         {:.0%}'.format(prediction[0][0]))
    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))
    print('Obscene:       {:.0%}'.format(prediction[0][2]))
    print('Threat:        {:.0%}'.format(prediction[0][3]))
    print('Insult:        {:.0%}'.format(prediction[0][4]))
    print('Identity Hate: {:.0%}'.format(prediction[0][5]))
    print()