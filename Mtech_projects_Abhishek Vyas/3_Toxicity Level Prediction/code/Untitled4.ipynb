{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id                                       comment_text  \\\n",
      "0   00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
      "1   0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
      "2   00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
      "3   00017563c3f7919a  :If you have a look back at the source, the in...   \n",
      "4   00017695ad8997eb          I don't anonymously edit articles at all.   \n",
      "5   0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
      "6   00024115d4cbde0f  Please do not add nonsense to Wikipedia. Such ...   \n",
      "7   000247e83dcc1211                   :Dear god this site is horrible.   \n",
      "8   00025358d4737918  \" \\n Only a fool can believe in such numbers. ...   \n",
      "9   00026d1092fe71cc  == Double Redirects == \\n\\n When fixing double...   \n",
      "10  0002eadc3b301559  I think its crap that the link to roggenbier i...   \n",
      "11  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...   \n",
      "12  0003806b11932181  , 25 February 2010 (UTC) \\n\\n :::Looking it ov...   \n",
      "13  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...   \n",
      "14  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...   \n",
      "15  000634272d0d44eb  ==Current Position== \\n Anyone have confirmati...   \n",
      "16  000663aff0fffc80                           this other one from 1897   \n",
      "17  000689dd34e20979  == Reason for banning throwing == \\n\\n This ar...   \n",
      "18  000834769115370c  :: Wallamoose was changing the cited material ...   \n",
      "19  000844b52dee5f3f             |blocked]] from editing Wikipedia.   |   \n",
      "\n",
      "    num_words  length  unique_words  num_punctuations  num_symbols  \\\n",
      "0          74     367            63                11            0   \n",
      "1          10      50             9                 2            0   \n",
      "2           5      54             5                 0            1   \n",
      "3          38     205            28                 5            0   \n",
      "4           7      41             7                 1            0   \n",
      "5          16      96            15                 2            0   \n",
      "6          28     176            26                 5            0   \n",
      "7           6      32             6                 2            0   \n",
      "8          99     556            72                13            0   \n",
      "9          39     224            33                 6            0   \n",
      "10         23     119            20                 2            0   \n",
      "11         66     448            54                10            0   \n",
      "12         46     293            40                 9            1   \n",
      "13         84     501            53                 9            0   \n",
      "14         54     334            36                 6            0   \n",
      "15         17     113            16                 1            0   \n",
      "16          5      24             5                 0            0   \n",
      "17         26     159            24                 4            0   \n",
      "18         63     336            48                10            0   \n",
      "19          4      38             4                 1            0   \n",
      "\n",
      "    num_stop_words  num_capitals  num_small  capital_proportions  \\\n",
      "0               30             4        280             1.089918   \n",
      "1                4             7         23            14.000000   \n",
      "2                1             4         22             7.407407   \n",
      "3               19             4        158             1.951220   \n",
      "4                3             1         32             2.439024   \n",
      "5                7             2         77             2.083333   \n",
      "6                9             5        135             2.840909   \n",
      "7                2             1         24             3.125000   \n",
      "8               34            42        341             7.553957   \n",
      "9               17             7        163             3.125000   \n",
      "10              13             2         93             1.680672   \n",
      "11              25            14        325             3.125000   \n",
      "12              19             8        200             2.730375   \n",
      "13              33            40        346             7.984032   \n",
      "14              22             4        247             1.197605   \n",
      "15               8             5         84             4.424779   \n",
      "16               3             0         16             0.000000   \n",
      "17              10             3        115             1.886792   \n",
      "18              27             8        252             2.380952   \n",
      "19               1             1         26             2.631579   \n",
      "\n",
      "    small_proportions  \n",
      "0           76.294278  \n",
      "1           46.000000  \n",
      "2           40.740741  \n",
      "3           77.073171  \n",
      "4           78.048780  \n",
      "5           80.208333  \n",
      "6           76.704545  \n",
      "7           75.000000  \n",
      "8           61.330935  \n",
      "9           72.767857  \n",
      "10          78.151261  \n",
      "11          72.544643  \n",
      "12          68.259386  \n",
      "13          69.061876  \n",
      "14          73.952096  \n",
      "15          74.336283  \n",
      "16          66.666667  \n",
      "17          72.327044  \n",
      "18          75.000000  \n",
      "19          68.421053  \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "data = pd.read_csv('jigsaw-toxic-comment-classification-challenge/test.csv')\n",
    "tokenizer = RegexpTokenizer(r'[\\w]+[-\\']?[\\w]*')\n",
    "#number of words\n",
    "temp = []\n",
    "temp1 = []\n",
    "num_punctuations = []\n",
    "unique_word = []\n",
    "num_symbols = []\n",
    "for i in data['comment_text']:\n",
    "    length = len(i)\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    dict = {}\n",
    "    for j in tokens:\n",
    "        if j in dict:\n",
    "            dict[j]+=1\n",
    "        else:\n",
    "            dict[j] = 1\n",
    "    unique_word+=[len(dict)]\n",
    "    temp+=[len(tokens)]\n",
    "    temp1+=[length]\n",
    "data['num_words'] = temp\n",
    "data['length'] = temp1\n",
    "data['unique_words'] = unique_word\n",
    "#print(data.head())\n",
    "for i in data['comment_text']:\n",
    "    count = 0\n",
    "    for j in i:\n",
    "        if(j==',' or j==';' or j==':' or j =='.'):\n",
    "            count+=1\n",
    "    num_punctuations+=[count]\n",
    "data['num_punctuations'] = num_punctuations\n",
    "\n",
    "for i in data['comment_text']:\n",
    "    count = 0\n",
    "    for j in i:\n",
    "        if(j=='*' or j=='$' or j=='&' or j =='%' or j=='@'):\n",
    "            count+=1\n",
    "    num_symbols+=[count]\n",
    "data['num_symbols'] = num_symbols\n",
    "#print(data.head(20))\n",
    "\n",
    "#num_stop_words\n",
    "num_stop_words = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in data['comment_text']:\n",
    "    count = 0\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    for j in tokens:\n",
    "        if j in stop_words:\n",
    "            count+=1\n",
    "    num_stop_words+=[count]\n",
    "data['num_stop_words'] = num_stop_words\n",
    "\n",
    "#Capital and small\n",
    "num_capital = []\n",
    "num_small = []\n",
    "prop_capital = []\n",
    "prop_small = []\n",
    "for i in data['comment_text']:\n",
    "    count = 0\n",
    "    count1 =0 \n",
    "    for j in i:\n",
    "        if(j.isupper()):\n",
    "            count+=1\n",
    "        if(j.islower()):\n",
    "            count1+=1\n",
    "    num_capital+=[count]\n",
    "    num_small+=[count1]\n",
    "data['num_capitals'] = num_capital\n",
    "data['num_small'] = num_small\n",
    "            \n",
    "#Proportions of capital and small\n",
    "cap_proportion = []\n",
    "small_proportion = []\n",
    "for i in range(0,len(data['comment_text'])):\n",
    "    prop = (data['num_capitals'][i]/data['length'][i])*100\n",
    "    sprop = (data['num_small'][i]/data['length'][i])*100\n",
    "    cap_proportion+=[prop]\n",
    "    small_proportion+=[sprop]\n",
    "\n",
    "data['capital_proportions'] = cap_proportion\n",
    "data['small_proportions'] = small_proportion\n",
    "print(data.head(20))  \n",
    "\n",
    "features = ('num_words','length','unique_words','num_punctuations','num_symbols','num_stop_words','num_capitals','num_small','capital_proportions','small_proportions')\n",
    "columns = ('toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate')\n",
    "# rows = [{c:data[f].corr(data[c]) for c in columns} for f in features]\n",
    "# data_correlations = pd.DataFrame(rows, index=features)\n",
    "# col = len(data_correlations.columns)\n",
    "# print(data_correlations.columns)\n",
    "# data_correlations\n",
    "# sum_prob = {}\n",
    "# for i in range(0,len(features)):\n",
    "#     sum_v = 0\n",
    "#     for j in range(0,col):\n",
    "#         sum_v+=data_correlations[columns[j]][features[i]]\n",
    "#     sum_prob[features[i]]=[sum_v]\n",
    "# print(sum_prob)\n",
    "\n",
    "# my_df = pd.DataFrame(data_correlations)\n",
    "# my_df.to_csv('my_csv_test.csv', index=features, header=columns)\n",
    "# my_df\n",
    "\n",
    "attributes = ('id','comment_text','num_words','length','unique_words','num_punctuations','num_symbols','num_stop_words','num_capitals','num_small','capital_proportions','small_proportions')\n",
    "df1 = pd.DataFrame(data)\n",
    "df1.to_csv('final_test.csv', index=False, header=attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
